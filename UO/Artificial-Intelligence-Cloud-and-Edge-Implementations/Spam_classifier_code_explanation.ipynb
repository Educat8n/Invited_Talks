{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_classifier_code_explanation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPh5F9xclr9eu+s48eFGV0Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amita-kapoor/Invited_Talks/blob/master/UO/Artificial-Intelligence-Cloud-and-Edge-Implementations/Spam_classifier_code_explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJWBf1wviFkV",
        "colab_type": "text"
      },
      "source": [
        "# Spam Classifier\n",
        "\n",
        "The Spam classifier classifies if a given text is spam or not.  \n",
        "\n",
        "We use the [UCI ML SMSSpam dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQoO7PnfI3PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The modules needed to run the code\n",
        "import argparse  # To read commandline argument and parse it\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import os  # For file and directory handling\n",
        "import shutil  # For file and directory handling\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix  #For measuring performance"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZzHU3sKJdYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some parameters\n",
        "DATA_DIR = \"data\"   # Data directory to save embedding\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")  # Numpy file containing word embeddings\n",
        "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"  # Dataset URL from where data is downloaded\n",
        "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"  # The gensim embedding model we will use\n",
        "EMBEDDING_DIM = 300  # The embedding dimensions\n",
        "NUM_CLASSES = 2  # The number of classes in output-- Spam or Ham\n",
        "BATCH_SIZE = 128  # The batch size\n",
        "NUM_EPOCHS = 3  # number of epochs for which model is to be trained\n",
        "\n",
        "\n",
        "# data distribution is 4827 ham and 747 spam (total 5574), which \n",
        "# works out to approx 87% ham and 13% spam, so we take reciprocals\n",
        "# and this works out to being each spam (1) item as being approximately\n",
        "# 8 times as important as each ham (0) message.\n",
        "CLASS_WEIGHTS = { 0: 1, 1: 8 }  # To take care of imbalance in classes\n",
        "\n",
        "tf.random.set_seed(42)  # Set the seed for random number generation to be able to reproduce results. "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUIj2eHJiLS8",
        "colab_type": "text"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "### Function download_and_read()\n",
        "This function takes a url as an argument using TF `get_file` function dwnloads the data from the given url, extracts it from the zip file and place it in folder `datasets`.\n",
        "#### tf.keras.utils.get_file()\n",
        "```\n",
        "tf.keras.utils.get_file(\n",
        "    fname, origin, untar=False, md5_hash=None, file_hash=None,\n",
        "    cache_subdir='datasets', hash_algorithm='auto', extract=False,\n",
        "    archive_format='auto', cache_dir=None\n",
        ")\n",
        "\n",
        "```\n",
        "You can learn more from the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMw9w0kgJQ6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Some utility functions-- should be ideally placed in a different python file --> can be util.py\n",
        "\n",
        "# Data downloading and data Processing\n",
        "\n",
        "\n",
        "\n",
        "def download_and_read(url):\n",
        "    \"\"\"\n",
        "    The function downloads the data from given url, splits it into Text and Labels\n",
        "    Uses tf.keras.utils.get_file() function to download the data from url--> function \n",
        "    downloads the data from the given url, extracts it from the zip file and place it in folder \"datasets\" \n",
        "    with the name specified in the first argument.\n",
        "    tf.keras.utils.get_file(\n",
        "    fname, origin, untar=False, md5_hash=None, file_hash=None,\n",
        "    cache_subdir='datasets', hash_algorithm='auto', extract=False,\n",
        "    archive_format='auto', cache_dir=None)\n",
        "\n",
        "    Arguments:\n",
        "    url: The url link of the dataset in zip format\n",
        "\n",
        "    Returns:\n",
        "    Two lists containing texts and respective labels\n",
        "\n",
        "    \"\"\"\n",
        "    local_file = url.split('/')[-1]  # split the file name (last string after '/') from url\n",
        "    p = tf.keras.utils.get_file(local_file, url, \n",
        "        extract=True, cache_dir=\".\")  #function to download the data from url to folder datasets with name given in local_file\n",
        "    labels, texts = [], []\n",
        "    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")  # define the path of the file from which to read data: datasets/SMSSpamCollection\n",
        "    with open(local_file, \"r\") as fin:\n",
        "        for line in fin:\n",
        "            label, text = line.strip().split('\\t')  # The labels and text are in one line separated by tab space.\n",
        "            labels.append(1 if label == \"spam\" else 0)\n",
        "            texts.append(text)\n",
        "    return texts, labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JxZyUR7jZwB",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings\n",
        "* Train your own embeddings - `scratch`\n",
        "* Use existing pre-trained embeddings\n",
        " * Word2Vec\n",
        " * GloVe- Global vectors for word representation - `vectorizer`\n",
        "* Fine Tune the pre-trained embeddings for your corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8vfK5iVjxVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We want to only consider embeddings for words that exist in our vocabulary. \n",
        "# So we make a smaller embedding matrix for each word in the vocabulary.\n",
        "# Each row in the matrix corresponds to a word, and the row itself is the vector corresponding \n",
        "# to the embedding for the word.\n",
        "# The function uses Gensim api to Download (if needed) dataset/model and load it to memory.\n",
        "\n",
        "def build_embedding_matrix(sequences, word2idx, embedding_dim, \n",
        "        embedding_file):\n",
        "    \"\"\"\n",
        "    The function reads the dict word2idx (word --> number) and written the corresponding\n",
        "    word vector for each word as defined by the Embedding model\n",
        "\n",
        "    Arguments:\n",
        "    sequences: not needed, not used-- just there because to suport back support for TF1 book\n",
        "    word2idx: Dictionary  containing words in the text and their respective idx as given by tokenizer.\n",
        "    embedding_dim: The number of units for the embedding layer\n",
        "    embedding_file: The data file in which embeddings will be store for future use.\n",
        "\n",
        "    \"\"\"\n",
        "    if os.path.exists(embedding_file):  # Checks if the embedding file already exists- then it justs loads it in the memory\n",
        "        E = np.load(embedding_file)\n",
        "    else:  # Else it creates the embedding file using the model specified in EMBEDDING_MODEL\n",
        "        vocab_size = len(word2idx)  # The vocabulary size is number of unique words in the text\n",
        "        E = np.zeros((vocab_size, embedding_dim)) # Creates a variable to store embeddings\n",
        "        word_vectors = api.load(EMBEDDING_MODEL)  # Get the embeddings from Gensim\n",
        "        for word, idx in word2idx.items():\n",
        "            try:\n",
        "                E[idx] = word_vectors.word_vec(word)  # For each word it converts it to respective word vector and store in Embedding file\n",
        "            except KeyError:   # word not in embedding\n",
        "                pass\n",
        "            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n",
        "            #     pass\n",
        "        np.save(embedding_file, E)  # The embeddings are saved in a file for future reference\n",
        "    return E"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPxnKlLBFTpN",
        "colab_type": "text"
      },
      "source": [
        "## Build Model\n",
        "\n",
        "Embedding Layer --> Dropout  --> Convolutional Layer 1D --> Pooling Layer --> Dense (Classification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWhlSac3Numj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next we build the model-- ideally model definition should also be in separate file --> model.py\n",
        "\n",
        "\n",
        "# We define a class SpamClassifierModel which builds and uses a 1D CNN to classify SMS texts as SPAM or HAM\n",
        "# Depending upon the mode selected- we either build the model from scratch, or use the word vectors given by Gensim API\n",
        "# or finetune the word vectors given by Gensim API\n",
        "\n",
        "class SpamClassifierModel(tf.keras.Model):  # The model is build using model API of Keras with tf.Keras.Model as the parent class. \n",
        "# The class inherits train, predict methods of the parent class.\n",
        "    def __init__(self, vocab_sz, embed_sz, input_length,\n",
        "            num_filters, kernel_sz, output_sz, \n",
        "            run_mode, embedding_weights, \n",
        "            **kwargs):\n",
        "        super(SpamClassifierModel, self).__init__(**kwargs)\n",
        "        if run_mode == \"scratch\":  # Choose the embedding layer scratch means the weights wil be traned from scratch\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                trainable=True)\n",
        "        elif run_mode == \"vectorizer\":  # Vectorizer means we use the pre-trained weights--> Transfer Learning\n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                weights=[embedding_weights],\n",
        "                trainable=False)\n",
        "        else:  # This is the fine tuning mode- we use pre-trained weights for the embedding layer and fine tune them. \n",
        "            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n",
        "                embed_sz,\n",
        "                input_length=input_length,\n",
        "                weights=[embedding_weights],\n",
        "                trainable=True)\n",
        "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)  # Add droput layer to avoid overfotting. \n",
        "        self.conv = tf.keras.layers.Conv1D(filters=num_filters,  # Define the 1D convolutional layer \n",
        "            kernel_size=kernel_sz,\n",
        "            activation=\"relu\")\n",
        "        self.pool = tf.keras.layers.GlobalMaxPooling1D()  # The pooling layer\n",
        "        self.dense = tf.keras.layers.Dense(output_sz, \n",
        "            activation=\"softmax\")  # And the last classifying layer consists of a fully connected Dense layer\n",
        "\n",
        "    def call(self, x):  # This function performs forward pass in the model. \n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCI4spRpeBH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The code below requires a folder to be created\n",
        "!mkdir data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeLFyTSKTrq_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "1da0958e-6751-4288-92dc-a2630a1f7344"
      },
      "source": [
        "## Now we will use the functions and model defined above --> ideally they should be done in a separate file-- main.py\n",
        "\n",
        "# read data\n",
        "texts, labels = download_and_read(DATASET_URL)\n",
        "\n",
        "# tokenize and pad text so that each text is of same size\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n",
        "num_records = len(text_sequences)\n",
        "max_seqlen = len(text_sequences[0])\n",
        "print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n",
        "\n",
        "# labels --> convert labels to categorical labels (one hot encoded)\n",
        "cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
        "\n",
        "# vocabulary --> Create word mapping and its inverse\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "word2idx[\"PAD\"] = 0\n",
        "idx2word[0] = \"PAD\"\n",
        "vocab_size = len(word2idx)\n",
        "print(\"vocab size: {:d}\".format(vocab_size))\n",
        "\n",
        "# load the dataset as tensors, split it into test, train and validation set\n",
        "dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n",
        "dataset = dataset.shuffle(10000)\n",
        "test_size = num_records // 4\n",
        "val_size = (num_records - test_size) // 10\n",
        "test_dataset = dataset.take(test_size)\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\n",
        "train_dataset = dataset.skip(test_size + val_size)\n",
        "\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Build the embedding\n",
        "E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n",
        "    EMBEDDING_NUMPY_FILE)\n",
        "print(\"Embedding matrix:\", E.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 0s 1us/step\n",
            "5574 sentences, max length: 189\n",
            "vocab size: 9010\n",
            "[===============================================---] 95.8% 360.5/376.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Embedding matrix: (9010, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwhSS-EobNwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Since we are not passing the mode by command line in this file we need to give a value to run_mode\n",
        "run_mode = 'scratch'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AysbP1aAWZgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "341f8145-9807-46f8-985d-e729ceb3f737"
      },
      "source": [
        "# Now we use the SpamClassifierModel class to create a model\n",
        "conv_num_filters = 256\n",
        "conv_kernel_size = 3\n",
        "model = SpamClassifierModel(\n",
        "    vocab_size, EMBEDDING_DIM, max_seqlen, \n",
        "    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
        "    run_mode, E)\n",
        "model.build(input_shape=(None, max_seqlen))\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"spam_classifier_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  2703000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              multiple                  230656    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  514       \n",
            "=================================================================\n",
            "Total params: 2,934,170\n",
            "Trainable params: 2,934,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRH7OGq9XW9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define  compile and train\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qr23sZJbL_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2d57febb-4fdc-4b15-b64f-e884d9b4d241"
      },
      "source": [
        "# Now we train the model\n",
        "model.fit(train_dataset, epochs=NUM_EPOCHS, \n",
        "    validation_data=val_dataset,\n",
        "    class_weight=CLASS_WEIGHTS)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "29/29 [==============================] - 21s 736ms/step - loss: 0.9490 - accuracy: 0.6711 - val_loss: 0.2146 - val_accuracy: 0.9896\n",
            "Epoch 2/3\n",
            "29/29 [==============================] - 21s 718ms/step - loss: 0.2932 - accuracy: 0.9704 - val_loss: 0.0552 - val_accuracy: 0.9792\n",
            "Epoch 3/3\n",
            "29/29 [==============================] - 21s 717ms/step - loss: 0.0915 - accuracy: 0.9898 - val_loss: 0.0539 - val_accuracy: 0.9818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f09a7b26c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJYZhnlFbgNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c89d436b-1df1-45ed-e8e2-21444bea0564"
      },
      "source": [
        "# Lastly we evaluate the trained model against test set\n",
        "labels, predictions = [], []\n",
        "for Xtest, Ytest in test_dataset:  \n",
        "    Ytest_ = model.predict_on_batch(Xtest)  # for each test test predict the label\n",
        "    ytest = np.argmax(Ytest, axis=1)  # Get the label with highest probabilty from actual test output\n",
        "    ytest_ = np.argmax(Ytest_, axis=1) # Get the label with highest probabilty from predictted test output\n",
        "    labels.extend(ytest.tolist())  # add to list\n",
        "    predictions.extend(ytest.tolist())  # add to list\n",
        "\n",
        "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))  # Calculate accuracy score\n",
        "print(\"confusion matrix\")\n",
        "print(confusion_matrix(labels, predictions))  # Calculate confusion matrix."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 1.000\n",
            "confusion matrix\n",
            "[[1091    0]\n",
            " [   0  189]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjg7mH1TH3Yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}